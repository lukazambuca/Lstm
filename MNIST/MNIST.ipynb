{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Handwritten Digit Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/intro.png\" width=\"600\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Statement**\n",
    "\n",
    "Here, we need to identify the digit in given images. We have total 70,000 images, out of which 49,000 are part of train images with the label of digit and rest 21,000 images are unlabeled (known as test images). Now, We need to identify the digit for test images. Public and Private split for test images are 40:60 and evaluation metric of this challenge is accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About the data**\n",
    "\n",
    "The MNIST dataset is an acronym that stands for the Modified National Institute of Standards and Technology datase.\n",
    "\n",
    "It is a dataset of 60,000 small square 28×28 pixel grayscale images of handwritten single digits between 0 and 9.\n",
    "\n",
    "<img src=\"Images/digits_separate.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "\n",
    "The task is to classify a given image of a handwritten digit into one of 10 classes representing integer values from 0 to 9, inclusively.\n",
    "\n",
    "Since each image has 28 by 28 pixels, we get a 28x28 array. We can flatten each array into a 28x28=784\n",
    "dimensional vector. Each component of the vector is a value between zero and one describing the intensity of the pixel (The input pixels are greyscale, with a value of 0.0 representing white, a value of 1.0 representing black, and in between values representing gradually darkening shades of grey). Thus, we generally think of MNIST as being a collection of 784-dimensional vectors.\n",
    "\n",
    "Not all vectors in this 784-dimensional space are MNIST digits. Typical points in this space are very different! To get a sense of what a typical point looks like, we can randomly pick a few points and examine them. In a random point – a random 28x28 image – each pixel is randomly black, white or some shade of gray. \n",
    "\n",
    "\n",
    "\n",
    "It is a widely used and deeply understood dataset and, for the most part, is “solved.” Top-performing models are deep learning convolutional neural networks that achieve a classification accuracy of above 99%, with an error rate between 0.4 %and 0.2% on the hold out test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why convlutional neural networks?\n",
    "\n",
    "The main structural feature of RegularNets is that all the neurons are connected to each other. For example, when we have images with 28 by 28 pixels with only greyscale, we will end up having 784 (28 x 28 x 1) neurons in a layer which seems manageable. However, most images have way more pixels and they are not grey-scaled. Therefore, assuming that we have a set of color images in 4K Ultra HD, we will have 26,542,080 (4096 x 2160 x 3) different neurons connected to each other in the first layer which is not really manageable. Therefore, we can say that RegularNets are not scalable for image classification. However, especially when it comes to images, there seems to be little correlation or relation between two individual pixels unless they are close to each other. This leads to the idea of Convolutional Layers and Pooling Layers.\n",
    "\n",
    "## About the layers \n",
    "\n",
    "### Convolutional layers\n",
    "\n",
    "Convolutional layer is the very first layer where we extract features from the images in our datasets. Due to the fact that pixels are only related with the adjacent and close pixels, convolution allows us to preserve the relationship between different parts of an image. Convolution is basically filtering the image with a smaller pixel filter to decrease the size of the image without loosing the relationship between pixels. When we apply convolution to 5x5 image by using a 3x3 filter with 1x1 stride (1 pixel shift at each step). We will end up having a 3x3 output (64% decrease in complexity).\n",
    "\n",
    "<img src=\"Images/ActivationMap.png\" width=\"600\" height=\"200\">\n",
    "\n",
    "For explanatory purposes, we initialize a 3x3 filter with ran values between 0 and 1. Then the first convulotional layer will be the dot product of the matrices from the input neuron (our image) and the filter. \n",
    "\n",
    "### Max Pooling\n",
    "\n",
    "When constructing CNNs, it is common to insert pooling layers after each convolution layer to reduce the spatial size of the representation to reduce the parameter counts which reduces the computational complexity. In addition, pooling layers also helps with the overfitting problem. Basically we select a pooling size to reduce the amount of the parameters by selecting the maximum, average, or sum values inside these pixels. Max Pooling, one of the most common pooling techniques, may be demonstrated as follows:\n",
    "\n",
    "<img src=\"Images/pooling.png\" width=\"400\" height=\"100\">\n",
    "\n",
    "## Fully connected layer\n",
    "\n",
    "Since our time-space complexity is vastly reduced thanks to convolution and pooling layers, we can construct a fully connected network in the end to classify our images.\n",
    "\n",
    " <img src=\"Images/fullconnected.jpeg\" width=\"900\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the images are all pre-aligned (e.g. each image only contains a hand-drawn digit), that the images all have the same square size of 28×28 pixels, and that the images are grayscale.\n",
    "\n",
    "Therefore, we can load the images and reshape the data arrays to have a single color channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, trainY), (testX, testY) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: X=(60000, 28, 28), y=(60000,)\n",
      "Test: X=(10000, 28, 28), y=(10000,)\n"
     ]
    }
   ],
   "source": [
    "print('Train: X=%s, y=%s' % (trainX.shape, trainY.shape))\n",
    "print('Test: X=%s, y=%s' % (testX.shape, testY.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick look at some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(9):\n",
    "# define subplot\n",
    "    pyplot.subplot(330 + 1 + i)\n",
    "# plot raw pixel data\n",
    "    pyplot.imshow(trainX[i], cmap=pyplot.get_cmap('gray'))\n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the images are all pre-aligned (e.g. each image only contains a hand-drawn digit), that the images all have the same square size of 28×28 pixels, and that the images are grayscale.\n",
    "Therefore, we can load the images and reshape the data arrays to have a single color channel.\n",
    "\n",
    " <img src=\"Images/numpy_array.png\" width=\"400\" height=\"300\">\n",
    " \n",
    " \n",
    "Keras Conv2D layer performs the convolution operation. It requires its input to be a 4-dimensional array. We have to reshape the input to ( , 1, 28, 28) or possibly to ( , 28, 28, 1), depending on your setup and backend (theano or tensorlow image layout convention).\n",
    "\n",
    "How would we store 60k images 28 by 28 pixels if it was RGB? For each pixel you would need 3 scalars (each for one channel), so it would be 60000x28x28x3.\n",
    "\n",
    "And how many channels you need when the image is in greyscale (in our case)? Just one, so it would be 60000x28x28x1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape dataset to have a single channel\n",
    "trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
    "testX = testX.reshape((testX.shape[0], 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trainX.shape[0] = 60 000\n",
    "trainX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hot one encoding\n",
    "\n",
    "We also know that there are 10 classes and that classes are represented as unique integers (0 to 9).\n",
    "\n",
    "We can, therefore, use a one hot encoding for the class element of each sample, transforming the integer into a 10 element binary vector with a 1 for the index of the class value, and 0 values for all other classes.\n",
    "\n",
    "<img src=\"Images/one-hot.png\" width=\"400\" height=\"300\">\n",
    "\n",
    "We can achieve this with the to_categorical() utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode target values\n",
    "trainY = to_categorical(trainY)\n",
    "testY = to_categorical(testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function that reshapes and hot-one encodes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Pixel Data\n",
    "\n",
    "We know that the pixel values for each image in the dataset are unsigned integers in the range between **black and white**, or **0 and 255.**\n",
    "\n",
    "<img src=\"Images/pixels.jpeg\" width=\"800\" height=\"500\">\n",
    "\n",
    "We do not know the best way to scale the pixel values for modeling, but we know that some scaling will be required.\n",
    "\n",
    "A good starting point is to normalize the pixel values of grayscale images, e.g. rescale them to the range **[0,1]**. This involves first converting the data type from unsigned integers to floats, then dividing the pixel values by the maximum value (255)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainX.astype('float32') / 255 \n",
    "\n",
    "testX = testX.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define a baseline convolutional neural network model for the problem.\n",
    "\n",
    "The model has two main aspects: \n",
    "\n",
    "1) Feature extraction front end comprised of convolutional and pooling layers\n",
    "\n",
    "2) Classifier backend that will make a prediction.\n",
    "\n",
    "For the convolutional front-end, we can start with a single convolutional layer with a small **filter size (3,3)** and a modest number of **filters (32)** followed by a max pooling layer. The filter maps can then be flattened to provide features to the classifier.\n",
    "\n",
    "Given that the problem is a multi-class classification task, we know that we will require an output layer with **10 nodes** in order to predict the probability distribution of an image belonging to each of the 10 classes. This will also require the use of a softmax activation function. Between the feature extractor and the output layer, we can add a dense layer to interpret the features, in this case with 100 nodes.\n",
    "\n",
    "All layers will use the **ReLU activation function** and the **weight initialization scheme**, both best practices. The aim of weight initialization is to prevent layer activation outputs from exploding or vanishing during the course of a forward pass through a deep neural network.\n",
    "\n",
    "We will use a conservative configuration for the **stochastic gradient descent optimizer** with a learning rate of 0.01 and a momentum of 0.9. The **categorical cross-entropy loss function** will be optimized, suitable for multi-class classification, and we will monitor the **classification accuracy metric**, which is appropriate given we have the same number of examples in each of the 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLu activation function\n",
    "\n",
    "The ReLU function is another non-linear activation function that has gained popularity in the deep learning domain. ReLU stands for **Rectified Linear Unit**. The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time.\n",
    "\n",
    "This means that the neurons will only be deactivated if the output of the linear transformation is less than 0. The plot below will help you understand this better-\n",
    "\n",
    "<img src=\"Images/relu.png\" width=\"600\" height=\"300\">\n",
    "\n",
    "\n",
    "ReLU is the most commonly used activation function in neural networks, especially in CNNs. If you are unsure what activation function to use in your network, ReLU is usually a good first choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "Stochastic gradient descent is a method to find the optimal parameter configuration for a machine learning algorithm. It iteratively makes small adjustments to a machine learning network configuration to decrease the error of the network. \n",
    " \n",
    "\n",
    "Error functions are rarely as simple as a typical parabola. Most often they have lots of hills and valleys, like the function pictured here. In this graph, if true gradient descent started on the left side of the graph, it would stop at the left valley because no matter which direction you travel from this point, you must travel upwards. This point is known as a local minimum. However, there exists another point in the graph that is lower. The lowest point in the entire graph is the global minimum, which is what stochastic gradient descent attempts to find. \n",
    "\n",
    "\n",
    "<img src=\"Images/stochastic-gradient-descent.jpg\" width=\"600\" height=\"300\">\n",
    "\n",
    "Stochastic gradient descent attempts to find the global minimum by adjusting the configuration of the network after each training point. Instead of decreasing the error, or finding the gradient,  for the entire data set, this method merely decreases the error by approximating the gradient for a randomly selected batch (which may be as small as single training sample).  In practice, the random selection is achieved by randomly shuffling the dataset and working through batches in a stepwise fashion.\n",
    "\n",
    "Heuristically,  if the network gets a training example wrong, it will update the configuration in favor of getting it right in the future. However, the configuration update might come at the cost of getting other questions wrong, thus increasing the overall error of the network. Thus not every training iteration may improve the network through the stochastic gradient descent algorithm.\n",
    " \n",
    "On the other hand, stochastic gradient descent can adjust the network parameters in such a way as to move the model out of a local minimum and toward a global minimum. Looking back to the concave function pictured above, after processing a training example, the algorithm may choose to move to the right on the graph in order to get out of the local minimum we were in. Even though doing so increases the error of the network, it allows it to move over the hill. This will allow further training to cause the gradient descent to move toward the global minimum. \n",
    " \n",
    "A benefit of stochastic gradient descent is that it requires much less computation than true gradient descent (and is therefore faster to calculate), while still generally converging to a minimum (although not necessarily a global one). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate\n",
    "\n",
    "In simple words learning rate determines how fast weights (in case of a neural network) or the cooefficents (in case of linear regression or logistic regression) change.\n",
    "\n",
    "If c is a cost function with variables (or weights) w1,w2….wn then,\n",
    "\n",
    "Lets take stochastic gradient descent where we change weights sample by sample -\n",
    "\n",
    "For every sample: w1new= w1 + (learning rate)* (derivative of cost function wrt w1\n",
    "\n",
    "<img src=\"Images/learning_rate.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "If learning rate is too high derivative may miss the 0 slope point or learning rate is too low then it may take forever to reach that point. So we need to figure out that balanced learning rate.\n",
    "\n",
    "<img src=\"Images/learning_rate2.png\" width=\"800\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "\n",
    "In neural networks, we use gradient descent optimization algorithm to minimize the error function to reach a global minima. In an ideal world the error function would look like this\n",
    "\n",
    "<img src=\"Images/momentum.gif\" width=\"200\" height=\"100\">\n",
    "\n",
    "So you are guaranteed to find the global optimum because there are no local minimum where your optimization can get stuck. However in real the error surface is more complex, may comprise of several local minima and may look like this\n",
    "\n",
    "<img src=\"Images/momentum2.gif\" width=\"200\" height=\"100\">\n",
    "\n",
    "In this case, you can easily get stuck in a local minima and the algorithm may think you reach the global minima leading to sub-optimal results. To avoid this situation, we use a momentum term in the objective function, which is a value between 0 and 1 that increases the size of the steps taken towards the minimum by trying to jump from a local minima. If the momentum term is large then the learning rate should be kept smaller. A large value of momentum also means that the convergence will happen fast. But if both the momentum and learning rate are kept at large values, then you might skip the minimum with a huge step. A small value of momentum cannot reliably avoid local minima, and can also slow down the training of the system. Momentum also helps in smoothing out the variations, if the gradient keeps changing direction. A right value of momentum can be either learned by hit and trial or through cross-validation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy metric\n",
    "\n",
    "Accuracy is one metric for evaluating classification models. Informally, accuracy is the fraction of predictions our model got right.\n",
    "\n",
    "<img src=\"Images/confusion_m.jpg\" width=\"600\" height=\"200\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally let's do some code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model and evaluate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 16s 271us/step - loss: 0.0960 - accuracy: 0.9726 - val_loss: 0.0814 - val_accuracy: 0.9755\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 17s 289us/step - loss: 0.0659 - accuracy: 0.9814 - val_loss: 0.0607 - val_accuracy: 0.9808\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 17s 291us/step - loss: 0.0510 - accuracy: 0.9855 - val_loss: 0.0583 - val_accuracy: 0.9811\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 18s 292us/step - loss: 0.0405 - accuracy: 0.9884 - val_loss: 0.0453 - val_accuracy: 0.9854\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 18s 295us/step - loss: 0.0350 - accuracy: 0.9901 - val_loss: 0.0457 - val_accuracy: 0.9840\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 18s 297us/step - loss: 0.0286 - accuracy: 0.9923 - val_loss: 0.0463 - val_accuracy: 0.9830\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 18s 296us/step - loss: 0.0246 - accuracy: 0.9934 - val_loss: 0.0423 - val_accuracy: 0.9854\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 18s 298us/step - loss: 0.0213 - accuracy: 0.9940 - val_loss: 0.0392 - val_accuracy: 0.9879\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 18s 299us/step - loss: 0.0185 - accuracy: 0.9951 - val_loss: 0.0389 - val_accuracy: 0.9859\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 18s 304us/step - loss: 0.0153 - accuracy: 0.9965 - val_loss: 0.0402 - val_accuracy: 0.9859\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 18s 304us/step - loss: 0.0132 - accuracy: 0.9974 - val_loss: 0.0363 - val_accuracy: 0.9864\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 18s 308us/step - loss: 0.0118 - accuracy: 0.9976 - val_loss: 0.0387 - val_accuracy: 0.9874\n",
      "Test loss: 0.03869649925859994\n",
      "Test accuracy: 0.9873999953269958\n"
     ]
    }
   ],
   "source": [
    "results = model.fit(trainX, trainY,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(testX, testY))\n",
    "\n",
    "score = model.evaluate(testX, testY, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 100)               540900    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 542,230\n",
      "Trainable params: 542,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining some of the code/terminology\n",
    "\n",
    "\n",
    "### model = Sequential()\n",
    "\n",
    "When to use a Sequential model:\n",
    "\n",
    "A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n",
    "\n",
    "A Sequential model is **not appropriate** when:\n",
    "\n",
    "    - Your model has multiple inputs or multiple outputs\n",
    "    - Any of your layers has multiple inputs or multiple outputs\n",
    "    - You need to do layer sharing\n",
    "    - You want non-linear topology (e.g. a residual connection, a multi-branch model)\n",
    "    \n",
    "You can also simply add layers via the .add() method\n",
    "\n",
    "### kernel_intializer\n",
    "\n",
    "Initializers define the way to set the initial random weights of Keras layers.\n",
    "\n",
    "The keyword arguments used for passing initializers to layers depends on the layer. Usually, it is simply kernel_initializer and bias_initializer.\n",
    "\n",
    "The neural network needs to start with some weights and then iteratively update them to better values. The term kernel_initializer is a fancy term for which statistical distribution or function to use for initialising the weights. In case of statistical distribut\n",
    "\n",
    "### Flatten\n",
    "\n",
    "<img src=\"Images/flatten.png\" width=\"600\" height=\"200\">\n",
    "\n",
    "### Dense\n",
    "\n",
    "Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True).\n",
    "\n",
    "### Batch size\n",
    "\n",
    "The batch size defines the number of samples that will be propagated through the network.\n",
    "\n",
    "For instance, let's say you have 1050 training samples and you want to set up a batch_size equal to 100. The algorithm takes the first 100 samples (from 1st to 100th) from the training dataset and trains the network. Next, it takes the second 100 samples (from 101st to 200th) and trains the network again. We can keep doing this procedure until we have propagated all samples through of the network. Problem might happen with the last set of samples. In our example, we've used 1050 which is not divisible by 100 without remainder. The simplest solution is just to get the final 50 samples and train the network.\n",
    "\n",
    "### Epoch\n",
    "\n",
    "This comes in the context of training a neural network with gradient descent. Since we usually train NNs using stochastic or mini-batch gradient descent, not all training data is used at each iterative step.\n",
    "\n",
    "Stochastic and mini-batch gradient descent use a batch_size number of training examples at each iteration, so at some point you will have used all data to train and can start over from the beginning of the dataset.\n",
    "\n",
    "Considering that, one epoch is one complete pass through the whole training set, means it is multiple iterations of gradient descent updates until you show all the data to the NN, and then start again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the first image in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f48278ab790>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Reload the data so that imshow can understand the input\n",
    "(_, _), (test_images, _) = mnist.load_data()\n",
    "\n",
    "pyplot.imshow(test_images[0], cmap=pyplot.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first image in the test set is the number **7**\n",
    "\n",
    "Since the output of the network was a layer with 10 units and a softmax activation, we will get an array of length 10 with a prediction for each potential number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.6656032e-10 2.1291169e-09 3.6747838e-07 2.1852575e-06 1.5035985e-11\n",
      " 1.5566966e-10 6.1807032e-14 9.9999750e-01 2.2503048e-10 3.3695716e-08]\n"
     ]
    }
   ],
   "source": [
    "print(preds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find the class with the highest prediction score with a numpy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(preds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model with K-fold\n",
    "\n",
    "After the model is defined, we need to evaluate it.\n",
    "\n",
    "The model will be evaluated using five-fold cross-validation. The value of k=5 was chosen to provide a baseline for both repeated evaluation and to not be so large as to require a long running time. Each test set will be 20% of the training dataset, or about 12,000 examples, close to the size of the actual test set for this problem.\n",
    "\n",
    "The training dataset is shuffled prior to being split, and the sample shuffling is performed each time, so that any model we evaluate will have the same train and test datasets in each fold, providing an apples-to-apples comparison between models.\n",
    "\n",
    "<img src=\"Images/kfolds.png\" width=\"600\" height=\"200\">\n",
    "\n",
    "We will train the baseline model for a modest 10 training epochs with a default batch size of 32 examples. The test set for each fold will be used to evaluate the model both during each epoch of the training run, so that we can later create learning curves, and at the end of the run, so that we can estimate the performance of the model. As such, we will keep track of the resulting history from each run, as well as the classification accuracy of the fold.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good resources\n",
    "\n",
    "**MNIST Data explained**\n",
    "\n",
    "https://colah.github.io/posts/2014-10-Visualizing-MNIST/ + https://www.youtube.com/watch?v=5gLarqG8p4s\n",
    "\n",
    "\n",
    "**MNIST Tutorial**\n",
    "\n",
    "https://wtfleming.github.io/2019/04/21/keras-mnist/\n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification/\n",
    "\n",
    "https://towardsdatascience.com/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d\n",
    "\n",
    "**About CNN**\n",
    "\n",
    "https://www.youtube.com/watch?v=YRhxdVk_sIs\n",
    "\n",
    "https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/\n",
    "\n",
    "**Max pooling**\n",
    "\n",
    "https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/\n",
    "\n",
    "https://www.youtube.com/watch?v=YRhxdVk_sIs\n",
    "\n",
    "\n",
    "**ReLu activation function**\n",
    "\n",
    "https://deepai.org/machine-learning-glossary-and-terms/relu\n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=Xehr9TBeJv4\n",
    "\n",
    "\n",
    "**Weight initilization**\n",
    "\n",
    " https://www.youtube.com/watch?v=YRhxdVk_sIs\n",
    " \n",
    " **Stochastic Gradient Descent**\n",
    " \n",
    " https://www.youtube.com/watch?v=vMh0zPT0tLI&vl=en\n",
    " \n",
    " https://deepai.org/machine-learning-glossary-and-terms/stochastic-gradient-descent\n",
    " \n",
    " **Momentum** \n",
    " \n",
    "https://www.willamette.edu/~gorr/classes/cs449/momrate.html\n",
    "\n",
    "https://www.quora.com/What-does-momentum-mean-in-neural-networks\n",
    "\n",
    "**Flatten**\n",
    "\n",
    "https://stackoverflow.com/questions/43237124/what-is-the-role-of-flatten-in-keras\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
